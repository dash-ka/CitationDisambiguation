{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Download and install Elasticsearch engine\n",
        "\n",
        "!pip install --upgrade pip\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "!tar -xzf /content/elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.9.2"
      ],
      "metadata": {
        "id": "TW3MCK3LYZIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "\n",
        "# Start Elasticsearch server and test the connection\n",
        "es_server = Popen(args=[\"elasticsearch-7.9.2/bin/elasticsearch\"], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda:os.setuid(1))\n",
        "# Wait until Elasticsearch has started\n",
        "!sleep 30\n",
        "!curl -X GET \"localhost:9200/?pretty\""
      ],
      "metadata": {
        "id": "tNUyisKHNvDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the document store.\n",
        "# Return the document embedding for later use with dense retriever\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "document_store = ElasticsearchDocumentStore(return_embedding=True)"
      ],
      "metadata": {
        "id": "P-q3rd2jZFhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading and processing \n",
        "\n",
        "!pip install datasets\n",
        "from collections import defaultdict\n",
        "from datasets import load_dataset, Dataset\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "5TA3DCZvaGoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data \n",
        "data = load_dataset(\"json\", data_files=\"/content/Bible.json\", split=\"train\")\n",
        "data.set_format(\"pandas\")\n",
        "data[\"version\"].value_counts()"
      ],
      "metadata": {
        "id": "FOjd4QTnJrfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset data to use only CEI1974 version\n",
        "cei = data[:]\n",
        "cei1974 = cei[cei.version ==\"CEI1974\"]\n",
        "df = Dataset.from_dict(cei1974)"
      ],
      "metadata": {
        "id": "-Ob1DUB2ez9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create documents to load in the document store. \n",
        "# Each document (a set of verses) has \"content\" field to store the content of the document,\n",
        "# and can have additional fields specified as a dictionary inside \"meta\" field. \n",
        "\n",
        "docs = []\n",
        "depth = 3 # Select the depth of the document search: max number of verses in a single document\n",
        "for book in df:\n",
        "  book_tuples = [(verse_dict[\"source\"], verse_dict[\"id\"]) for verse_dict in book[\"segments\"]]\n",
        "  for i in tqdm(range(len(book_tuples))):\n",
        "    docs.append({\"content\":book_tuples[i][0], \"meta\":{\"id\":book_tuples[i][1]}})   \n",
        "    if i < len(book_tuples)-1:  \n",
        "      for step in range(1, depth):\n",
        "        docs.append({\"content\":\" \".join([tpl[0] for tpl in book_tuples[i:(i+1+step)]]), \\\n",
        "                     \"meta\":{\"id\":\" \".join([tpl[1] for tpl in book_tuples[i:(i+1+step)]])}})"
      ],
      "metadata": {
        "id": "nC3S8ryY_mC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing documents to database (the operation can take several minutes)\n",
        "document_store.write_documents(docs)\n",
        "document_store.get_document_count()"
      ],
      "metadata": {
        "id": "UGbhNT89gls6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Retriever with document_store\n",
        "from haystack.nodes import EmbeddingRetriever\n",
        "\n",
        "retriever = EmbeddingRetriever(\n",
        "    document_store=document_store,\n",
        "    embedding_model=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
        "    model_format=\"sentence_transformers\",\n",
        ")\n",
        "# We need to call update_embeddings() to iterate over all\n",
        "# previously indexed documents and update their embedding representation.\n",
        "# While this can be a time consuming operation, it only needs to be done once.\n",
        "# At query time, we only need to embed the query and compare it to the existing document embeddings.\n",
        "document_store.update_embeddings(retriever)"
      ],
      "metadata": {
        "id": "FzoRJhgiiKEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to search and retrieve verses from the Bible\n",
        "# having high semantic similarity with some input text.\n",
        "\n",
        "def ricerca_versetto(text, top_k=3):\n",
        "  \"\"\"\n",
        "  Retrieve the identifier of the verse in the document store which is closest to the input text\n",
        "  and return the corresponding semantic similarity score.\n",
        "  Args:\n",
        "    text (str): a text to match against documents inside the document store\n",
        "    top_k (int): a number of documents to retrieve from the document store\n",
        "  Returns:\n",
        "    A dictionary with the identifier of the matched biblic verse, \n",
        "    the text of the matched verse and its matching score.\n",
        "  \"\"\"\n",
        "  retrieved_docs = retriever.retrieve(query=text, top_k=top_k)\n",
        "  output = []\n",
        "  for doc in retrieved_docs:\n",
        "    output.append({\"id_vestetto\":doc.meta[\"id\"],\n",
        "                   \"versetto\":doc.content,\n",
        "                   \"semantic_similarity\": round(doc.score, 5)})\n",
        "  return output"
      ],
      "metadata": {
        "id": "xKPuCCGzi7j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"perchÃ© siete stati arricchiti di tutti i doni, compresi quello della parola e quello della conoscenza\"\n",
        "ricerca_versetto(query, top_k=3)"
      ],
      "metadata": {
        "id": "UPEUvS0ZlRWD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}